version: '3.8'

services:
  llama-cpp:
    build: 
      context: "https://github.com/ggerganov/llama.cpp.git#master"
      dockerfile: .devops/full-cuda.Dockerfile
      args:
        - CUDA_VERSION=12.2.0
    command: >-
      --server
      --model /models/airoboros-l2-13b-2.2.1.Q6_K.gguf
      --threads 12
      --batch-size 512
      --ctx-size 4096
      --mlock
      --n-gpu-layers 33
      --main-gpu 0
      --host 0.0.0.0
      --port 8080
      --timeout 1000
    ports:
      - '8080:8080'
    volumes:
      - './data/models:/models'
    ulimits:
      memlock: -1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
